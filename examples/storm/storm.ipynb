{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORM\n",
    "\n",
    "[STORM](https://arxiv.org/abs/2402.14207) is a research assistant designed by Shao, et. al that extends the idea of \"outline-driven RAG\" for richer article generation.\n",
    "\n",
    "STORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles:\n",
    "\n",
    "1. Creating an outline (planning) by querying similar topics helps improve coverage.\n",
    "2. Multi-perspective, grounded (in search) conversation simulation helps increase the reference count and information density. \n",
    "\n",
    "The control flow looks like the diagram below.\n",
    "\n",
    "![STORM diagram](./img/storm.png)\n",
    "\n",
    "STORM имеет несколько основных этапов:\n",
    "\n",
    "1. Создание первоначальной структуры + Обзор связанных предметов\n",
    "2. Определение различных точек зрения\n",
    "3. \"Интервью с экспертами по предмету\" (ролевые игры LLM)\n",
    "4. Уточнение структуры (используя ссылки)\n",
    "5. Написание разделов, затем написание статьи\n",
    "\n",
    "Этап интервью с экспертами происходит между ролевым автором статьи и исследовательским экспертом. \"Эксперт\" может запрашивать внешние знания и отвечать на конкретные вопросы, сохраняя цитируемые источники в векторном хранилище, так что на более поздних этапах уточнения можно синтезировать полную статью.\n",
    "\n",
    "Есть несколько гиперпараметров, которые вы можете установить для ограничения (потенциально) бесконечной ширины исследований:\n",
    "\n",
    "N: Количество перспектив для обзора / использования (Шаги 2->3)\n",
    "M: Максимальное количество ходов разговора на этапе (Шаг 3)\n",
    "\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U gigachain_community gigachain_openai langgraph wikipedia  scikit-learn\n",
    "# We use one or the other search engine below\n",
    "# %pip install -U duckduckgo tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to draw the pretty graph diagrams.\n",
    "# If you are on MacOS, you will need to run brew install graphviz before installing and update some environment flags\n",
    "# ! brew install graphviz\n",
    "# !CFLAGS=\"-I $(brew --prefix graphviz)/include\" LDFLAGS=\"-L $(brew --prefix graphviz)/lib\" pip install -U pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if os.environ.get(var):\n",
    "        return\n",
    "    os.environ[var] = getpass.getpass(var + \":\")\n",
    "\n",
    "\n",
    "# Set for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"STORM\"\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select LLMs\n",
    "\n",
    "We will have a faster LLM do most of the work, but a slower, long-context model to distill the conversations and write the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "long_context_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Initial Outline\n",
    "\n",
    "For many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial\n",
    "outline to be refined after our research. Below, we will use our \"fast\" llm to generate the outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Вы - автор Википедии. Напишите структуру страницы Википедии на заданную пользователем тему. Будьте всесторонними и конкретными.\",\n",
    "        ),\n",
    "        (\"user\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
    "\n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and descriptions for each section of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "\n",
    "\n",
    "generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Большие языковые модели в России\n",
      "\n",
      "## Введение\n",
      "\n",
      "Общее понятие о больших языковых моделях и их значимость в области искусственного интеллекта.\n",
      "\n",
      "## История\n",
      "\n",
      "Развитие больших языковых моделей в России, ключевые этапы и достижения.\n",
      "\n",
      "## Применение\n",
      "\n",
      "Области применения больших языковых моделей в России, включая обработку естественного языка, машинный перевод, генерацию текста и другие.\n",
      "\n",
      "## Технологии\n",
      "\n",
      "Описание основных технологий и методов, используемых для создания и обучения больших языковых моделей в России.\n",
      "\n",
      "## Вызовы и перспективы\n",
      "\n",
      "Актуальные вызовы, с которыми сталкиваются разработчики больших языковых моделей в России, а также перспективы и направления развития данной области.\n"
     ]
    }
   ],
   "source": [
    "example_topic = \"Большие языковые модели в России\"\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "\n",
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Topics\n",
    "\n",
    "While language models do store some Wikipedia-like knowledge in their parameters, you will get better results by incorporating relevant and recent information using a search engine.\n",
    "\n",
    "We will start our search by generating a list of related topics, sourced from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \n",
    "\"\"\"Я пишу страницу Википедии по упомянутой ниже теме. Пожалуйста, определите и порекомендуйте некоторые страницы Википедии по тесно связанным предметам. Я ищу примеры, которые предоставляют информацию о интересных аспектах, обычно ассоциируемых с этой темой, или примеры, которые помогут мне понять типичное содержание и структуру страниц Википедии для похожих тем.\n",
    "\n",
    "Пожалуйста, перечислите как можно больше предметов и URL-адресов.\n",
    "\n",
    "Интересующая тема: {topic}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"Comprehensive list of related subjects as background research.\",\n",
    "    )\n",
    "\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(\n",
    "    RelatedSubjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['Большие языковые модели', 'Искусственный интеллект', 'Машинное обучение', 'Технологии обработки естественного языка', 'Инновации в области искусственного интеллекта в России'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Perspectives\n",
    "\n",
    "From these related subjects, we can select representative Wikipedia editors as \"subject matter experts\" with distinct\n",
    "backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor.\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"Comprehensive list of editors with their roles and affiliations.\",\n",
    "        # Add a pydantic validation/restriction to be at most M editors\n",
    "    )\n",
    "\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Вам нужно выбрать разнообразную (и различную) группу редакторов Википедии, которые будут работать вместе над созданием всесторонней статьи по теме. Каждый из них представляет разную точку зрения, роль или принадлежность, связанную с этой темой.\\\n",
    "    Вы можете использовать страницы Википедии по смежным темам для вдохновения. Для каждого редактора добавьте описание того, на чем они будут сосредоточены.\n",
    "\n",
    "    Структуры страниц Википедии по смежным темам для вдохновения:\n",
    "    {examples}\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Интересующая тема: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ").with_structured_output(Perspectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata[\"categories\"])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[\n",
    "        :max_length\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str):\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Russian AI Researcher',\n",
       "   'name': 'Natalia Ivanova',\n",
       "   'role': 'Researcher',\n",
       "   'description': 'Natalia specializes in the development and application of large language models in the Russian context. She focuses on the technical aspects, advancements, and challenges of implementing such models in Russia.'},\n",
       "  {'affiliation': 'Russian Tech Journalist',\n",
       "   'name': 'Alexei Petrov',\n",
       "   'role': 'Journalist',\n",
       "   'description': 'Alexei covers the latest news and trends related to large language models in Russia. He focuses on the societal impact, ethical considerations, and implications of these models on various sectors.'},\n",
       "  {'affiliation': 'Russian Government Official',\n",
       "   'name': 'Olga Volkova',\n",
       "   'role': 'Policy Maker',\n",
       "   'description': 'Olga is involved in policymaking related to AI technologies in Russia, including regulations and guidelines concerning the use of large language models. She focuses on ensuring ethical and legal compliance in their implementation.'},\n",
       "  {'affiliation': 'Russian Language Educator',\n",
       "   'name': 'Dmitri Ivanov',\n",
       "   'role': 'Educator',\n",
       "   'description': 'Dmitri specializes in language education and is interested in how large language models can be integrated into language learning curricula in Russia. He focuses on the educational benefits and challenges of using such models.'},\n",
       "  {'affiliation': 'Russian Tech Entrepreneur',\n",
       "   'name': 'Sergei Smirnov',\n",
       "   'role': 'Entrepreneur',\n",
       "   'description': 'Sergei is a tech entrepreneur who is exploring business opportunities related to large language models in Russia. He focuses on the commercial applications, market trends, and potential innovations in this field.'}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Dialog\n",
    "\n",
    "Now the true fun begins, each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second \"domain expert\" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.\n",
    "\n",
    "\n",
    "### Interview State\n",
    "\n",
    "The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own \"persona\") to make it easy to parallelize these conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dialog Roles\n",
    "\n",
    "The graph will have two participants: the wikipedia editor (`generate_question`), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain), who uses a search engine to answer the questions as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Вы опытный автор Википедии и хотите отредактировать конкретную страницу.\n",
    "Кроме вашей идентичности как писателя Википедии, у вас есть конкретный фокус при исследовании темы.\n",
    "Теперь вы общаетесь с экспертом, чтобы получить информацию. Задавайте хорошие вопросы, чтобы получить больше полезной информации.\n",
    "\n",
    "Когда у вас не останется вопросов, скажите \"Большое спасибо за вашу помощь!\", чтобы завершить разговор.\n",
    "Пожалуйста, задавайте по одному вопросу за раз и не спрашивайте то, что уже спрашивали.\n",
    "Ваши вопросы должны быть связаны с темой, о которой вы хотите написать.\n",
    "Будьте всесторонними и любопытными, получая как можно больше уникальных сведений от эксперта.\\\n",
    "\n",
    "Оставайтесь верны своей конкретной перспективе:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str):\n",
    "    converted = []\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    return {\"messages\": converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state[\"editor\"]\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Да, верно. Я специализируюсь на разработке и применении больших языковых моделей в российском контексте. Мой фокус - это технические аспекты, достижения и проблемы внедрения таких моделей в России. Если у вас есть вопросы на эту тему, я с удовольствием помогу.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f\"Итак, вы говорите, что пишете статью на тему {example_topic}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[0],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "question[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer questions\n",
    "\n",
    "The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Исчерпывающий список запросов поисковой системы для ответа на вопросы пользователя.\",\n",
    "    )\n",
    "\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Вы - полезный ассистент исследователя. Используйте поисковую систему, чтобы ответить на вопросы пользователя.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ").with_structured_output(Queries, include_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['технические аспекты применения больших языковых моделей',\n",
       " 'достижения в области языковых моделей в России',\n",
       " 'проблемы внедрения языковых моделей в России']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "queries[\"parsed\"].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\",\n",
    "    )\n",
    "    cited_urls: List[str] = Field(\n",
    "        description=\"List of urls cited in the answer.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
    "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "\n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Вы эксперт, умеющий эффективно использовать информацию. Вы общаетесь с автором Википедии, который хочет\n",
    "написать страницу Википедии по теме, которую вы знаете. Вы собрали связанную информацию и теперь используете эту информацию для формирования ответа.\n",
    "\n",
    "Сделайте ваш ответ максимально информативным и убедитесь, что каждое предложение подкреплено собранной информацией.\n",
    "Каждый ответ должен быть подтвержден цитированием из надежного источника, оформленным как сноска, с воспроизведением URL-адресов после вашего ответа.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ").with_config(run_name=\"GenerateAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "# Tavily is typically a better search engine, but your free queries are limited\n",
    "# search_engine = TavilySearchResults(max_results=4, tavily_api_key=)\n",
    "\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "import json\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: RunnableConfig | None = None,\n",
    "    name: str = \"Subject Matter Expert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries[\"parsed\"].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries[\"raw\"]\n",
    "    tool_call = queries[\"raw\"].additional_kwargs[\"tool_calls\"][0]\n",
    "    tool_id = tool_call[\"id\"]\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В разработке и применении больших языковых моделей в России ключевую роль играют технические аспекты, включая методы оценки моделей, тестирование на людях и другие метрики[^1]. Достижения внедрения языковых моделей в России включают разработку сложных чат-ботов и виртуальных ассистентов на базе LLM[^2]. Одной из проблем внедрения языковых моделей является обнаружение смещения и его смягчение, так как это важные аспекты для LLM[^3]. В России начали тестировать отечественные языковые модели для внедрения в сервис \"Госуслуги\"[^4]. NLP-технологии активно внедряются в жизнь, в том числе на основе больших языковых моделей, таких как ChatGPT[^5].\\n\\nCitations:\\n\\n[1]: https://www.unite.ai/ru/\\nоценка-больших-языковых-моделей,-техническое-руководство/\\n[2]: https://www.unite.ai/ru/\\nбольшие-языковые-модели/\\n[3]: https://sysblok.ru/knowhow/kak-rabotajut-jazykovye-modeli/\\n[4]: https://www.forbes.ru/tekhnologii/505447-kommersant-uznal-o-testirovanii-rossijskih-azykovyh-modelej-dla-gosuslug\\n[5]: https://www.rbc.ru/industries/news/65140b169a7947a25a22401d'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Interview Graph\n",
    "\n",
    "\n",
    "Now that we've defined the editor and domain expert, we can compose them in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject Matter Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Большое спасибо за вашу помощь!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# Feel free to comment out if you have\n",
    "# not installed pygraphviz\n",
    "# Image(interview_graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_question\n",
      "--  [AIMessage(content='Да, именно. Моя специализация связана с разработкой и применением больших языковых моделей в российском контексте. Я интересуюсь техническими аспектами, достижениями и вызовами при внедрении таких моделей в России. У меня есть несколько вопросов по этой теме. Какие компании или и\n",
      "answer_question\n",
      "--  [AIMessage(content='Несколько компаний и исследовательских групп в России активно занимаются разработкой и применением больших языковых моделей. Например, Sber AI Research под руководством Дениса разработал русскоязычные text2image трансформерные модели ruDALL-E и ряд других моделей. Компания Полен \n",
      "ask_question\n",
      "--  [AIMessage(content='Благодарю вас за информацию об активности компаний и исследовательских групп в России в области разработки и применения больших языковых моделей. Какие основные технические вызовы или проблемы связаны с разработкой и применением больших языковых моделей в российском контексте?', \n",
      "answer_question\n",
      "--  [AIMessage(content='Одним из основных технических вызовов при разработке и применении больших языковых моделей в российском контексте является передача обучения, которая является ключевой концепцией в разработке LLM. Она включает обучение модели на большом наборе данных. Также важным аспектом являет\n",
      "ask_question\n",
      "--  [AIMessage(content='Спасибо за подробное описание технических вызовов при разработке и применении больших языковых моделей в российском контексте. Какие инновационные методы или подходы используются в России для преодоления этих вызовов и улучшения эффективности больших языковых моделей?', name='Nat\n",
      "answer_question\n",
      "--  [AIMessage(content='В России для преодоления вызовов и улучшения эффективности больших языковых моделей используются инновационные методы, такие как обучение больших языковых моделей, тонкая настройка LLM, адаптация моделей к уникальным требованиям, использование методов Low-Rank Adaptation (LoRA) и\n",
      "ask_question\n",
      "--  [AIMessage(content='Спасибо за информацию об инновационных методах, используемых в России для преодоления вызовов и улучшения эффективности больших языковых моделей. Какие перспективы развития вы видите для применения больших языковых моделей в России в будущем?', name='Natalia Ivanova')]\n",
      "answer_question\n",
      "--  [AIMessage(content='Проспекты развития применения больших языковых моделей в России в будущем включают в себя увеличение интереса к этой технологии, трансформацию различных отраслей с использованием LLM, внедрение мультимодальных возможностей в модели, а также продолжение исследований и разработок в\n",
      "__end__\n",
      "--  [AIMessage(content='Итак, вы говорите, что пишите статью на тему Большие языковые модели в России?', name='Subject Matter Expert'), AIMessage(content='Да, именно. Моя специализация связана с разработкой и применением больших языковых моделей в российском контексте. Я интересуюсь техническими аспекта\n"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"Итак, вы говорите, что пишите статью на тему {example_topic}?\",\n",
    "            name=\"Subject Matter Expert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "    if END in step:\n",
    "        final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = next(iter(final_step.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Outline\n",
    "\n",
    "At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Вы - автор Википедии. Вы собрали информацию от экспертов и поисковых систем. Теперь вы уточняете структуру страницы Википедии.\n",
    "Вам нужно убедиться, что структура всесторонняя и конкретная.\n",
    "Тема, о которой вы пишете: {topic}\n",
    "\n",
    "Старая структура:\n",
    "\n",
    "{old_outline}\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Уточните структуру на основе ваших разговоров с экспертами по предмету:\\n\\nРазговоры:\\n\\n{conversations}\\n\\nНапишите уточненную структуру Википедии:\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Большие языковые модели в России\n",
      "\n",
      "## Введение\n",
      "\n",
      "Общее понятие о больших языковых моделях и их значимость в области искусственного интеллекта, особенно в контексте российского рынка и науки.\n",
      "\n",
      "## История развития\n",
      "\n",
      "Описание ключевых этапов развития и важных достижений в области больших языковых моделей в России, с акцентом на ведущие исследовательские группы и компании.\n",
      "\n",
      "## Активные участники\n",
      "\n",
      "Перечень компаний и исследовательских организаций в России, активно занимающихся разработкой и применением больших языковых моделей, включая краткое описание их вклада.\n",
      "\n",
      "## Технологии и методы\n",
      "\n",
      "Обзор основных технологий, методов и инновационных подходов, используемых в разработке и улучшении больших языковых моделей в России.\n",
      "\n",
      "## Применение\n",
      "\n",
      "Анализ областей применения больших языковых моделей в России, включая обработку естественного языка, машинный перевод, генерацию текста и др.\n",
      "\n",
      "## Вызовы и решения\n",
      "\n",
      "Обзор актуальных технических вызовов при разработке и применении больших языковых моделей в России и обсуждение различных методов для их преодоления.\n",
      "\n",
      "## Перспективы развития\n",
      "\n",
      "Описание будущих направлений развития и применения больших языковых моделей в России, включая потенциальное влияние на различные отрасли.\n"
     ]
    }
   ],
   "source": [
    "print(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Article\n",
    "\n",
    "Now it's time to generate the full article. We will first divide-and-conquer, so that each section can be tackled by an individual llm. Then we will prompt the long-form LLM to refine the finished article (since each section may use an inconsistent voice).\n",
    "\n",
    "#### Create Retriever\n",
    "\n",
    "The research process uncovers a large number of reference documents that we may want to query during the final article-writing process.\n",
    "\n",
    "First, create the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "# This really doesn't need to be a vectorstore for this size of data.\n",
    "# It could just be a numpy matrix. Or you could store documents\n",
    "# across requests if you want.\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Руководитель группы Sber AI Research. Под руководством Дениса были обучены первые русскоязычные text2image трансформерные модели ruDALL-E 1.3B (Malevich) и ruDALL-E 12B (Kandinsky 1.0), диффузионные модели Kandinsky 2.0, 2.1, 2.2, Kandinsky 3.0, а также модель ...', metadata={'id': 'f9b3c05f-e1eb-4de8-af6a-d6914d530b87', 'source': 'https://opentalks.ai/ru/speakers'}),\n",
       " Document(page_content='Изучите преобразующее влияние больших языковых моделей, таких как Med-PaLM 2 от Google и Meditron от EPFL, в здравоохранении, изучая их применение, проблемы и потенциал в улучшении ухода за пациентами и клинической эффективности.', metadata={'id': '117f8a34-6602-4517-9daa-8781751716d3', 'source': 'https://www.unite.ai/ru/revolutionizing-healthcare-exploring-the-impact-and-future-of-large-language-models-in-medicine/'}),\n",
       " Document(page_content='4 марта 2024Hi-TECH Academy. Аналитик данных. 5 марта 2024SF Education. Бизнес-аналитик. 5 марта 2024SF Education. Больше курсов на Хабр Карьере. Хочу представить вам пересказ-обзор на статью Large Language Model based Multi-Agents: A ...', metadata={'id': '1f1c7360-05c9-40ee-bc26-e45bbe779ef0', 'source': 'https://habr.com/ru/articles/796555/'}),\n",
       " Document(page_content='Кроме того, экспоненциально множатся случаи применения больших языковых моделей в целях написания различных видов письменных работ, в том числе выпускных в школах и вузах.', metadata={'id': 'ac954eba-9628-4987-b6d8-d1a8cfd7c8d5', 'source': 'https://russiancouncil.ru/analytics-and-comments/analytics/tak-li-strashen-chatgpt-i-analogichnye-emu-bolshie-yazykovye-modeli/'})]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Зачем нужен RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sections\n",
    "\n",
    "Now you can generate the sections using the indexed docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        title=\"Full content of the subsection. Include [#] citations to the cited sources where relevant.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
    "\n",
    "\n",
    "class WikiSection(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    content: str = Field(..., title=\"Full content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "    citations: List[str] = Field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            subsection.as_str for subsection in self.subsections or []\n",
    "        )\n",
    "        citations = \"\\n\".join([f\" [{i}] {cit}\" for i, cit in enumerate(self.citations)])\n",
    "        return (\n",
    "            f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
    "            + f\"\\n\\n{citations}\".strip()\n",
    "        )\n",
    "\n",
    "\n",
    "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Вы - опытный автор Википедии. Завершите ваш назначенный WikiSection из следующей структуры:\\n\\n\"\n",
    "            \"{outline}\\n\\nЦитируйте ваши источники, используя следующие ссылки:\\n\\n<Документы>\\n{docs}\\n<Документы>\",\n",
    "        ),\n",
    "        (\"user\", \"Напишите полный WikiSection для раздела {section}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n",
    "\n",
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt\n",
    "    | long_context_llm.with_structured_output(WikiSection)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## История развития\n",
      "\n",
      "Развитие больших языковых моделей (БЯМ) в России тесно связано с глобальными тенденциями в области искусственного интеллекта (ИИ), но также содержит уникальные аспекты, отражающие национальные научные интересы и технологические приоритеты. Начиная с ранних исследований в сфере обработки естественного языка (NLP) и машинного обучения, Россия постепенно занимала своё место на международной арене ИИ, разрабатывая и внедряя передовые технологии в этой области.\n",
      "\n",
      "Первые значимые шаги были сделаны в 2000-х годах, когда ведущие университеты и исследовательские институты начали активно изучать и применять методы машинного обучения для обработки русского языка. Этот период был отмечен появлением первых компьютерных программ, способных анализировать и обрабатывать текст на русском языке с использованием базовых алгоритмов NLP.\n",
      "\n",
      "С развитием технологий и увеличением вычислительных мощностей начался переход к созданию более сложных и мощных моделей. В 2010-х годах в России были разработаны первые большие языковые модели, способные генерировать текст, осуществлять машинный перевод и выполнять другие сложные задачи обработки языка. Этот период также был отмечен активным сотрудничеством между академическими кругами и промышленностью, что способствовало более широкому внедрению ИИ-технологий в экономику страны.\n",
      "\n",
      "Ключевым моментом в развитии БЯМ в России стало появление и последующее развитие национальных проектов и инициатив, направленных на поддержку исследований и разработок в области ИИ, включая большие языковые модели. Эти проекты обеспечили необходимую инфраструктуру и финансирование для проведения масштабных исследований, что способствовало значительному прорыву в качестве и масштабе разработанных моделей. Сегодня Россия продолжает активно развивать и внедрять новые технологии в области БЯМ, оставаясь одним из лидеров в этой области на международной арене.[0] https://anns.ru/articles/news/2023/07/25/5_podhodov_k_otsenke_bolshih_jazikovih_modeley\n"
     ]
    }
   ],
   "source": [
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": example_topic,\n",
    "    }\n",
    ")\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate final article\n",
    "\n",
    "Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Вы экспертный автор Википедии. Напишите полную статью для Википедии на тему {topic}, используя следующие черновики разделов:\\n\\n\"\n",
    "            \"{draft}\\n\\nСтрого следуйте руководствам по форматированию Википедии.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Напишите полную статью Википедии, используя формат markdown. Организуйте ссылки с помощью сносок вида \"[1]\",'\n",
    "            ' избегая дублирования в подвале статьи. Включите URL-адреса в подвал.',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Большие языковые модели в России\n",
      "\n",
      "Большие языковые модели (БЯМ) в России — это область исследования и разработки в сфере искусственного интеллекта (ИИ), направленная на создание алгоритмов, способных понимать, генерировать и анализировать естественный язык на уровне, сравнимом с человеческим. Эти модели находят широкое применение в различных областях, от автоматического перевода и систем автоматического ответа до анализа текстов и создания контента.\n",
      "\n",
      "## Содержание\n",
      "- [История развития](#история-развития)\n",
      "- [Ведущие исследовательские центры](#ведущие-исследовательские-центры)\n",
      "- [Применение и влияние](#применение-и-влияние)\n",
      "- [Текущие тенденции и будущее развитие](#текущие-тенденции-и-будущее-развитие)\n",
      "- [Ссылки](#ссылки)\n",
      "\n",
      "## История развития\n",
      "\n",
      "Развитие больших языковых моделей в России тесно связано с глобальными тенденциями в области ИИ, однако имеет уникальные аспекты, отражающие национальные научные интересы и технологические приоритеты. Начиная с ранних 2000-х, когда ведущие университеты и исследовательские институты начали активно изучать методы машинного обучения для обработки русского языка, Россия постепенно укрепляла свои позиции в международном ИИ-сообществе.\n",
      "\n",
      "Первые значительные успехи были достигнуты в 2010-х годах с созданием моделей, способных выполнять сложные задачи обработки языка. Этот период также отмечен активным сотрудничеством между академическими кругами и промышленностью, что способствовало более широкому внедрению ИИ-технологий в экономику страны. Появление национальных проектов и инициатив в области ИИ обеспечило необходимую поддержку для дальнейшего развития и исследований в области БЯМ.\n",
      "\n",
      "## Ведущие исследовательские центры\n",
      "\n",
      "В России существует ряд ведущих исследовательских центров и университетов, которые играют ключевую роль в разработке и изучении больших языковых моделей:\n",
      "- Институт проблем передачи информации РАН\n",
      "- Московский физико-технический институт\n",
      "- Санкт-Петербургский государственный университет\n",
      "- Национальный исследовательский университет \"Высшая школа экономики\"\n",
      "\n",
      "Эти организации не только проводят передовые исследования в области ИИ и БЯМ, но и обеспечивают подготовку квалифицированных специалистов в этой сфере.\n",
      "\n",
      "## Применение и влияние\n",
      "\n",
      "Большие языковые модели находят применение в различных сферах, таких как автоматический перевод, создание контента, анализ текстов и обработка естественного языка. В России разработки в этой области активно используются для повышения эффективности образовательных процессов, оптимизации рабочих процессов в компаниях и улучшения пользовательского опыта в интернет-сервисах.\n",
      "\n",
      "## Текущие тенденции и будущее развитие\n",
      "\n",
      "Современные исследования в области БЯМ в России сосредотачиваются на улучшении качества и точности моделей, а также на расширении их функциональности. Особое внимание уделяется развитию технологий глубокого обучения и нейронных сетей. В будущем ожидается дальнейшее интегрирование БЯМ в различные отрасли экономики, что позволит автоматизировать рутинные задачи, повысить качество обслуживания и создать новые продукты и услуги.\n",
      "\n",
      "## Ссылки\n",
      "\n",
      "- [Подходы к оценке больших языковых моделей](https://anns.ru/articles/news/2023/07/25/5_podhodov_k_otsenke_bolshih_jazikovih_modeley)\n",
      "\n",
      "[К началу](#большие-языковые-модели-в-россии)"
     ]
    }
   ],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Финальный Процесс\n",
    "\n",
    "Теперь пришло время связать все воедино. У нас будет 6 основных этапов, следующих друг за другом:\n",
    ".\n",
    "1. Создание первоначальной структуры + точек зрения\n",
    "2. Партийное общение с каждой точкой зрения для расширения содержания статьи\n",
    "3. Уточнение структуры на основе разговоров\n",
    "4. Индексация справочных документов из бесед\n",
    "5. Написание отдельных разделов статьи\n",
    "6. Написание финальной версии вики\n",
    "\n",
    "Состояние отслеживает результаты каждого этапа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"Итак, вы пишите статью на тему {topic}?\",\n",
    "                    name=\"Subject Matter Expert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
    "    return f'Обсуждение с {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image(storm.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_research\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год.', subsections=None), Section(section_title='Основные направления', description='Основные стратегические нап\n",
      "conduct_interviews\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год.', subsections=None), Section(section_title='Основные направления', description='Основные стратегические нап\n",
      "refine_outline\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год, включая основные цели и принципы.', subsections=None), Section(section_title='Основные направления', descri\n",
      "index_references\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год, включая основные цели и принципы.', subsections=None), Section(section_title='Основные направления', descri\n",
      "write_sections\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год, включая основные цели и принципы.', subsections=None), Section(section_title='Основные направления', descri\n",
      "write_article\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год, включая основные цели и принципы.', subsections=None), Section(section_title='Основные направления', descri\n",
      "__end__\n",
      "--  {'topic': 'Стратегия Сбера 2026', 'outline': Outline(page_title='Стратегия Сбера 2026', sections=[Section(section_title='Введение', description='Общая информация о стратегии Сбера на 2026 год, включая основные цели и принципы.', subsections=None), Section(section_title='Основные направления', descri\n"
     ]
    }
   ],
   "source": [
    "async for step in storm.astream(\n",
    "    {\n",
    "        \"topic\": \"Стратегия Сбера 2026\",\n",
    "    }\n",
    "):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name])[:300])\n",
    "    if END in step:\n",
    "        results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the Wiki\n",
    "\n",
    "Now we can render the final wiki page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Стратегия Сбера 2026\n",
       "\n",
       "### Содержание\n",
       "- [Введение](#Введение)\n",
       "- [Основные направления](#Основные-направления)\n",
       "- [Стратегические действия и цели](#Стратегические-действия-и-цели)\n",
       "  - [Компания \"Сбер\"](#Компания-\"Сбер\")\n",
       "  - [Исследовательские организации](#Исследовательские-организации)\n",
       "- [Инновационные технологии и сервисы](#Инновационные-технологии-и-сервисы)\n",
       "  - [Примеры внедрения](#Примеры-внедрения)\n",
       "- [Влияние на трудовую сферу](#Влияние-на-трудовую-сферу)\n",
       "  - [Позитивные аспекты](#Позитивные-аспекты)\n",
       "  - [Негативные аспекты](#Негативные-аспекты)\n",
       "- [Заключение](#Заключение)\n",
       "- [Ссылки](#Ссылки)\n",
       "\n",
       "### Введение\n",
       "\n",
       "Большие языковые модели (БЯМ) в последние годы стали одним из наиболее заметных достижений в области искусственного интеллекта (ИИ). Эти модели обучаются на огромных объемах текстовых данных, что позволяет им улавливать тонкости языка, его структуру и контекстуальные связи. В России интерес к разработке и применению БЯМ активно растет, подчеркивая их важность не только для научного сообщества, но и для промышленных и коммерческих приложений.\n",
       "\n",
       "### Основные направления\n",
       "\n",
       "Основные направления исследований и разработок в России включают обработку естественного языка (NLP), машинный перевод, генерацию текста, анализ данных и майнинг текста, а также развитие ИИ в различных сферах деятельности.\n",
       "\n",
       "### Стратегические действия и цели\n",
       "\n",
       "#### Компания \"Сбер\"\n",
       "\n",
       "\"Сбер\" акцентирует внимание на развитии технологий ИИ, включая БЯМ, как ключевое направление своей стратегии развития до 2026 года. Компания планирует инвестиции в новые продукты и услуги, основанные на ИИ, для улучшения пользовательского опыта и создания новых возможностей для клиентов и партнеров[1].\n",
       "\n",
       "#### Исследовательские организации\n",
       "\n",
       "Многие исследовательские организации в России сосредоточены на создании новых технологий в области ИИ и БЯМ. Они стремятся к научному прогрессу и имеют практическое применение в различных секторах[2].\n",
       "\n",
       "### Инновационные технологии и сервисы\n",
       "\n",
       "Российские компании стремятся интегрировать инновационные технологии, в том числе БЯМ, для улучшения пользовательского опыта и повышения эффективности работы. Сбербанк планирует активно инвестировать в развитие ИТ-сектора, что позволит предложить новые улучшенные сервисы[3].\n",
       "\n",
       "#### Примеры внедрения\n",
       "\n",
       "Сбербанк, как крупнейший российский банк, активно инвестирует в развитие передовых технологий, включая БЯМ, что позволяет значительно улучшить качество обслуживания клиентов и увеличить производительность труда[4].\n",
       "\n",
       "### Влияние на трудовую сферу\n",
       "\n",
       "БЯМ оказывают значительное влияние на трудовую сферу, автоматизируя рутинные задачи и создавая спрос на специалистов в области ИИ. Однако это также может привести к сокращению рабочих мест в некоторых секторах и требует программ переподготовки[5].\n",
       "\n",
       "#### Позитивные аспекты\n",
       "\n",
       "Внедрение БЯМ способствует повышению эффективности работы и развитию инновационных продуктов и услуг, создавая новые рабочие места в сфере высоких технологий[6].\n",
       "\n",
       "#### Негативные аспекты\n",
       "\n",
       "Потенциальное сокращение рабочих мест из-за автоматизации процессов и повышенные требования к квалификации работников могут привести к увеличению уровня безработицы среди неквалифицированных работников[7].\n",
       "\n",
       "### Заключение\n",
       "\n",
       "Большие языковые модели в России играют ключевую роль в развитии ИИ, способствуя научному и промышленному прогрессу. Стратегия Сбера до 2026 года подчеркивает значимость этих технологий, стремясь к созданию более эффективных и инновационных продуктов. Развитие и применение БЯМ в России будут иметь значительное влияние не только на научное сообщество, но и на экономику в целом.\n",
       "\n",
       "### Ссылки\n",
       "[0] https://lenta.ru/articles/2023/12/07/2026/\n",
       "\n",
       "[1] https://www.sb.by/articles/sber-predstavil-strategiyu-razvitiya-do-2026-goda.html\n",
       "\n",
       "[2] https://www.rbc.ru/industries/news/656ddead9a794783f03a9640\n",
       "\n",
       "[3] https://www.comnews.ru/content/230576/2023-12-07/2023-w49/1008/sber-pokazal-ambicii-novoy-strategii-razvitiya\n",
       "\n",
       "[4] https://www.rbc.ru/industries/news/65716ff99a7947f66891fd84\n",
       "\n",
       "[5] https://lenta.ru/articles/2023/12/07/2026/\n",
       "\n",
       "[6] https://www.rbc.ru/industries/news/656ddead9a794783f03a9640\n",
       "\n",
       "[7] https://www.sb.by/articles/sber-predstavil-strategiyu-razvitiya-do-2026-goda.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# We will down-header the sections to create less confusion in this notebook\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
